The time since deposition (TSD) of a bloodstain, i.e., the time of a bloodstain formation is an essential piece of biological evidence in crime scene investigation. The practical usage of some existing microscopic methods (e.g., spectroscopy or RNA analysis technology) is limited, as their performance strongly relies on high-end instrumentation and/or rigorous laboratory conditions. This paper presents a practically applicable deep learning-based method (i.e., BloodNet) for efficient, accurate, and costless TSD inference from a macroscopic view, i.e., by using easily accessible bloodstain photos. To this end, we established a benchmark database containing around 50,000 photos of bloodstains with varying TSDs. Capitalizing on such a large-scale database, BloodNet adopted attention mechanisms to learn from relatively high-resolution input images the localized fine-grained feature representations that were highly discriminative between different TSD periods. Also, the visual analysis of the learned deep networks based on the Smooth Grad-CAM tool demonstrated that our BloodNet can stably capture the unique local patterns of bloodstains with specific TSDs, suggesting the efficacy of the utilized attention mechanism in learning fine-grained representations for TSD inference. As a paired study for BloodNet, we further conducted a microscopic analysis using Raman spectroscopic data and a machine learning method based on Bayesian optimization. Although the experimental results show that such a new microscopic-level approach outperformed the state-of-the-art by a large margin, its inference accuracy is significantly lower than BloodNet, which further justifies the efficacy of deep learning techniques in the challenging task of bloodstain TSD inference. Our code is publically accessible via https://github.com/shenxiaochenn/BloodNet. Our datasets and pre-trained models can be freely accessed via https://figshare.com/articles/dataset/21291825.
This paper solves the problem of learning dense visual correspondences between different object instances of the same category with only sparse annotations. We decompose this pixel-level semantic matching problem into two easier ones: (i) First, local feature descriptors of source and target images need to be mapped into shared semantic spaces to get coarse matching flows. (ii) Second, matching flows in low resolution should be refined to generate accurate point-to-point matching results. We propose asymmetric feature learning and matching flow super-resolution based on vision transformers to solve the above problems. The asymmetric feature learning module exploits a biased cross-attention mechanism to encode token features of source images with their target counterparts. Then matching flow in low resolutions is enhanced by a super-resolution network to get accurate correspondences. Our pipeline is built upon vision transformers and can be trained in an end-to-end manner. Extensive experimental results on several popular benchmarks, such as PF-PASCAL, PF-WILLOW, and SPair-71 K, demonstrate that the proposed method can catch subtle semantic differences in pixels efficiently. Code is available on https://github.com/YXSUNMADMAX/ACTR.
This paper presents our work to the Expression Classification Challenge of the 5th Affective Behavior Analysis in-the-wild (ABAW) Competition. In our method, the multi-modal features are extracted by several different pertained models, which are used to build different combinations to capture more effective emotion information. Specifically, we extracted efficient facial expression features using MAE encoder pre-trained with a large-scale face dataset. For these combinations of visual and audio modal features, we utilize two kinds of temporal encoders to explore the temporal contextual information in the data. In addition, we employ several ensemble strategies for different experimental set-tings to obtain the most accurate expression recognition results. Our system achieves the average F1 Score of 0.4072 on the test set of Aff-wild2 ranking 2nd, which proves the effectiveness of our method.
Time series aomaly detection has been widely studied in recent years. Previous research focuses on point-wise features and pairwise associations for feature learning or designed anomaly scores based on prior knowledge. However, these methods cannot fully learn the intricate abnormal dynamic information and can only identify a limited class of anomalies. We propose a Masked Attention Network with Query Sparsity Measurement (MAN-QSM) to address the above challenges. This model uses two kinds of prior knowledge to fully exploit the differences between normal and abnormal points from two perspectives: pairwise association and sequence-level information. We designs the anomaly mask mechanism to collaborate with the training strategy to amplify the difference between normal and abnormal points. In experiments, we compare the model with classical methods, reconstruction-based models, autoregressive-based models, and state-of-the-art models, and the MAN-QSM achieves state-of-the-art results on SMD, PSM, and MSL datasets with an average of 16% reduction in error rate.
Carbon storage capacity can be estimated to establish evaluation standards and statistics for carbon neutrality. Existing estimation methods including machine learning system have weakness modeling ability, and they are unable to deal with the complex topographies and temporal changes in vegetation in urban zones. However, a deep neural network has the potential ability to face such complex scenes because of its nonlinear fitting properties which has been widely used in industry. In this study, a novel and powerful neural network learning system named CiSL-NPP is proposed, to integrate multi-source data as an accurate, efficient, simple, long-term, city-scale carbon storage capacity measurement method. We use an unsupervised generation model spatio-temporal Masked AutoEncoder (st-MAE) and lightweight RNN model to efficiently obtain high-quality data that contain time-series information such as seasons; this minimizes any undue impact of meteorological, temporal, and spatial factors on the measurement owing to the multi-source, multi-modal nature of the data. In the MAE, we add seasonal coding to make it time-series sensitive; we also embed road network information to accurately perceive the complex topography of the city. Results for 16 cities in China and Europe revealed that the proposed method shows: 1) Higher-quality generated data (MSE is 0.13-0.29); 2) Accurate coverage of time series and complex geographical features; 3) Satisfaction of estimation demands with only 316 RMB cost; 4) Capability to evolve a long-term trend of urban vegetation carbon storage capacity in 4.2 days; and 5) Easily interpretable results which could, in practice, provide sound guidance for urban planning and decision-making processes.
Despite the significant improvements that self-supervised representation learning has led to when learning from unlabeled data, no methods have been developed that explain what influences the learned representation. We address this need through our proposed approach, RELAX, which is the first approach for attribution-based explanations of representations. Our approach can also model the uncertainty in its explanations, which is essential to produce trustworthy explanations. RELAX explains representations by measuring similarities in the representation space between an input and masked out versions of itself, providing intuitive explanations that significantly outperform the gradient-based baselines. We provide theoretical interpretations of RELAX and conduct a novel analysis of feature extractors trained using supervised and unsupervised learning, providing insights into different learning strategies. Moreover, we conduct a user study to assess how well the proposed approach aligns with human intuition and show that the proposed method outperforms the baselines in both the quantitative and human evaluation studies. Finally, we illustrate the usability of RELAX in several use cases and highlight that incorporating uncertainty can be essential for providing faithful explanations, taking a crucial step towards explaining representations.
Deep learning algorithms are widely used for pattern recognition in electronic noses, which are sensor arrays for gas mixtures. One of the challenges of using electronic noses is sensor drift, which can degrade the accuracy of the system over time, even if it is initially trained to accurately estimate concentrations from sensor data. In this paper, an effective drift compensation method is introduced that adds sensor drift information during training of a neural network that estimates gas concentrations. This is achieved by concatenating a calibration feature vector with sensor data and using this as an input to the neural network. The calibration feature vector is generated via a masked-autoencoder-based feature extractor trained with transfer samples, and acts as a prompt to convey sensor drift information. Our method is tested on a 3-year gas sensor array drift dataset, showing that a neural network using our method performs better than other models, including a network with additional fine tuning, demonstrating that our method is efficient at compensating for sensor drift. In this study, the effectiveness of using prompts for network training is confirmed, which better compensates for drifts in new sensor signals than network fine-tuning.
The remote-sensing (RS) community has shown increasing interest in self-supervised learning for its ability to learn representations without labeled data. These representations can be easily adapted to downstream tasks through pretraining and fine-tuning. Recently, masked autoencoders (MAEs) have achieved better semantic representation by masking out a significant portion of the input image. However, the original design of MAEs for RGB natural images may not be optimal for RS images, which exhibit considerable variation between modalities like synthetic aperture radar (SAR) and optical. To address this, we propose a 3-D mask (3DM) that enhances feature extraction along the vertical dimension. After fine-tuning, our 3DMAE model outperforms state-of-the-art (SOTA) contrastive and MAE-based models on BigEarthNet-MM classification and significantly reduces the input data volume by at least 50% with the vertical mask, resulting in a more efficient model. Generalization experiments show a 5.9% F1-score improvement when applied to the SEN12MS dataset, which has diverse data distributions.
Identifying the right user to target is a common problem for different Internet platforms. Although numerous systems address this task, they are heavily tailored for specific environments and settings. It is challenging for practitioners to apply these findings to their problems. The reason is that most systems are designed for settings with millions of highly active users and with personal information, as is the case in social networks or other services with high virality. There exists a gap in the literature for systems that are for medium-sized data and where the only data available are the event sequences of a user. It motivates us to present Look-A-Liker (LAL) as an unsupervised deep cluster system. It uses temporal point processes to identify similar users for targeting tasks. We use data from the leading Internet marketplace for the gastronomic sector for experiments. LAL generalizes beyond proprietary data. Using event sequences of users, it is possible to obtain state-of-the-art results compared to novel methods such as Transformer architectures and multimodal learning. Our approach produces the up to 20% ROC AUC score improvement on real-world datasets from 0.803 to 0.959. Although LAL focuses on hundreds of thousands of sequences, we show how it quickly expands to millions of user sequences. We provide a fully reproducible implementation with code and datasets in https://github.com/adasegroup/sequence_clusterers.
Deep neural networks typically require accurate and a large number of annotations to achieve outstanding performance in medical image segmentation. One-shot and weakly-supervised learning are promising research directions that reduce labeling effort by learning a new class from only one annotated image and using coarse labels instead, respectively. In this work, we present an innovative framework for 3D medical image segmentation with one-shot and weakly-supervised settings. Firstly a propagation-reconstruction network is proposed to propagate scribbles from one annotated volume to unlabeled 3D images based on the assumption that anatomical patterns in different human bodies are similar. Then a multi-level similarity denoising module is designed to refine the scribbles based on embeddings from anatomical- to pixel-level. After expanding the scribbles to pseudo masks, we observe the miss-classified voxels mainly occur at the border region and propose to extract self-support prototypes for the specific refinement. Based on these weakly-supervised segmentation results, we further train a segmentation model for the new class with the noisy label training strategy. Experiments on three CT and one MRI datasets show the proposed method obtains significant improvement over the state-of-the-art methods and performs robustly even under severe class imbalance and low contrast. Code is publicly available at https://github.com/LWHYC/OneShot_WeaklySeg .
