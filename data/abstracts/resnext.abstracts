Multiple sclerosis (MS) is one of the most common inflammatory neurological diseases in young adults. There are three types of MS: (1) In relapsing remitting MS (RRMS), people have temporarily periods of relapses (attacks) for days or weeks, and then symptoms seem to disappear (remitting stage). (2) In secondary progressive MS (SPMS), symptoms worsen more steadily over time. Attacks (relapses) may occur time to time but the disease can progress in non-attack periods. It is estimated that half of the RRMS patients progress to SPMS in 10 years. (3) Primary progressive MS (PPMS) is characterized by slowly worsening symptoms from the beginning, with no relapses or remissions. For PPMS patients, disability progresses slowly. Researchers have found out that in the first year, MS causes more damage than following 5–10 years. Therefore, early diagnosis is vital. In this context, deep learning models started to be popular for assisting identification/diagnosis/classification of MS patients using magnetic resonance imaging (MRI). This paper provides an in-depth review of deep learning approaches for identification and classification of MS using brain MRI images. We discuss recent trends of deep learning methods for MS identification under three categories: CNN models, hybrid models (CNN with a classifier) and deep transfer learning models. Existing deep learning algorithms are analyzed and compared according to their architecture, image modality, pre-processing, feature extraction, classifier, dataset, categories and accuracy. This survey paper would provide a valuable source for researchers who are interested in state-of-the-art deep learning methods for MS identification using MRI images.
Age-related macular degeneration (AMD) is the leading cause of visual impairment among elderly in the world. Early detection of AMD is of great importance, as the vision loss caused by this disease is irreversible and permanent. Color fundus photography is the most cost-effective imaging modality to screen for retinal disorders. Cutting edge deep learning based algorithms have been recently developed for automatically detecting AMD from fundus images. However, there are still lack of a comprehensive annotated dataset and standard evaluation benchmarks. To deal with this issue, we set up the Automatic Detection challenge on Age-related Macular degeneration (ADAM), which was held as a satellite event of the ISBI 2020 conference. The ADAM challenge consisted of four tasks which cover the main aspects of detecting and characterizing AMD from fundus images, including detection of AMD, detection and segmentation of optic disc, localization of fovea, and detection and segmentation of lesions. As part of the ADAM challenge, we have released a comprehensive dataset of 1200 fundus images with AMD diagnostic labels, pixel-wise segmentation masks for both optic disc and AMD-related lesions (drusen, exudates, hemorrhages and scars, among others), as well as the coordinates corresponding to the location of the macular fovea. A uniform evaluation framework has been built to make a fair comparison of different models using this dataset. During the ADAM challenge, 610 results were submitted for online evaluation, with 11 teams finally participating in the onsite challenge. This paper introduces the challenge, the dataset and the evaluation methods, as well as summarizes the participating methods and analyzes their results for each task. In particular, we observed that the ensembling strategy and the incorporation of clinical domain knowledge were the key to improve the performance of the deep learning models.
Recent studies estimate human anatomical key points through the single monocular image, in which multichannel heatmaps are the key factor in determining the quality of human pose estimation. Multichannel heatmaps can efficiently handle the image-to-coordinate mapping task and the processing of semantic features. Most methods ignore physical constraints and internal relationships of human body parts, which easily misclassify left and right symmetrical parts as similar features. Some studies use RNNs on the top to incorporate priors about the structure of pose components and body configuration. Therefore, a novel top-down convolutional network is proposed to consider these priors during training, which can improve the robustness under complex field conditions in the wild. In order to learn the prior knowledge of human pose configuration, the hierarchy of fully convolutional networks (discriminator) is used to distinguish real poses from fake ones. Consequently, the pose network is inclined to make a pose estimation that the discriminator misjudges as true, which is reasonable in complex situations. The performance of the method is experimentally validated by pose estimation on the MS COCO human key point detection task. The proposed approach outperforms the original method and generates robust pose predictions, demonstrating efficiency by using adversarial learning.
Lightweight CNN models aim to extend the application of deep learning from conventional image classification to mobile edge device-based image classification. However, the accuracy of lightweight CNN models currently is not as comparable as traditional large CNN models. To improve the accuracy of mobile platform-based image classification, we propose MobileACNet, a novel ACNet-based lightweight model based on MobileNetV3 (a popular lightweight CNN for image classification on mobile platforms). Our model adopts a similar idea to ACNet: consider global inference and local inference adaptively to improve the classification accuracy. We improve the MobileNetV3 by replacing the inverted residual block with our proposed adaptive inverted residual module (AIR). Experimental results show that our proposed MobileACNet can effectively improve the image classification accuracy by providing additional adaptive global inference on three public datasets, i.e., Cifar-100 dataset, Tiny ImageNet dataset, and a large-scale dataset ImageNet, for mobile-platform-based image classification.
We present a novel network, PI-Net, for the fusion between point clouds and images in this letter. Most existing fusion methods project point clouds into pseudo images and then fuse the pseudo and RGB images with 2D CNNs. To get rid of structuring the pseudo images as the preprocessing, we propose a new fusion module, PI-fusion module. It can directly fuse points with pixels or grids which refer to the cells in the feature maps extracted from RGB images. Specifically, every point is aligned with a certain pixel/grid based on the pre-calibrated camera-LiDAR external parameters. The features aggregated from the points and pixels/grids are transferred to each other, which achieves the bidirectional fusion of the information. Then, we construct the PI-Net's fusion backbone by plugging the PI-fusion modules between a certain 2D CNN and a point-based network to fuse the features at every scale. The detection and the segmentation head networks further utilize the fused information to complete various tasks. The PI-Net unifies the point-cloud-to-image and image-to-point-cloud fusion into one network and achieves state-of-the-art results on the various tasks, such as the road detection, 3D object detection, and point cloud segmentation tasks on the KITTI benchmark.
There are three main disadvantages including time-consuming task, high cost and complex detection procedures in the semen quality measurement to heighten the roosters’ reproductive capability in breeder flocks. Another solution is to select the breeder roosters with fine phenotypic characteristics by humans, while it is also a considerably labor-intensive task and even increases the risk of zoonoses at a poultry farm. To solve these problems, this paper proposes a strategy that effective promoting factors applied to Progressive Multi-Granularity (PMG) network ensures the accuracy of entire image and improves the accuracy of fine-grained image. This strategy allows the basic networks boost the classification performance in the case of specific combination. Given the same images inputted into our model, two groups of questionnaires for practitioners and non-practitioners judging the fertility by the rooster’s phenotypic traits, the experimental results show that our method has raised the accuracy by almost 10% by comparison with the results of questionnaire survey.
Biofouling is the accumulation of organisms on surfaces immersed in water. It is of particular concern to the international shipping industry because it increases fuel costs and presents a biosecurity risk by providing a pathway for non-indigenous marine species to establish in new areas. There is growing interest within jurisdictions to strengthen biofouling risk-management regulations, but it is expensive to conduct in-water inspections and assess the collected data to determine the biofouling state of vessel hulls. Machine learning is well suited to tackle the latter challenge, and here we apply deep learning to automate the classification of images from in-water inspections to identify the presence and severity of fouling. We combined several datasets to obtain over 10,000 images collected from in-water surveys which were annotated by a group biofouling experts. We compared the annotations from three experts on a 120-sample subset of these images, and found that they showed 89% agreement (95% CI: 87–92%). Subsequent labelling of the whole dataset by one of these experts achieved similar levels of agreement with this group of experts, which we defined as performing at most 5% worse (p 0.009–0.054). Using these expert labels, we were able to train a deep learning model that also agreed similarly with the group of experts (p 0.001–0.014), demonstrating that automated analysis of biofouling in images is feasible and effective using this method.
Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) are two primary frameworks for current semantic image recognition tasks in the community of computer vision. The general consensus is that both CNNs and ViT have their latent strengths and weaknesses, e.g., CNNs are good at extracting local features but difficult to aggregate long-range feature dependencies, while ViT is good at aggregating long-range feature dependencies but poorly represents in local features. In this paper, we propose an auxiliary and integrated network architecture, named Convolutional-Auxiliary Efficient Graph Reasoning Transformer (CAE-GReaT), which joints strengths of both CNNs and ViT into a uniform framework. CAE-GReaT stands on the shoulders of the advanced graph reasoning transformer and employs an internal auxiliary convolutional branch to enrich the local feature representations. Besides, to reduce the computational costs in graph reasoning, we also propose an efficient information diffusion strategy. Compared to the existing ViT models, CAE-GReaT not only has the advantage of a purposeful interaction pattern (via the graph reasoning branch), but also can capture fine-grained heterogeneous feature representations (via the auxiliary convolutional branch). Extensive experiments are implemented on three challenging dense image prediction tasks, i.e., semantic segmentation, instance segmentation, and panoptic segmentation. Results demonstrate that CAE-GReaT can achieve consistent performance gains on the state-of-the-art baselines with a slightly computational cost.
Modern deep networks generally implement a certain form of shortcut connections to alleviate optimization difficulties. However, we observe that such network topology alters the nature of deep networks. In many ways, these networks behave similarly to aggregated wide networks. We thus exploit the aggregation nature of shortcut connections at a finer architectural level and place them within wide convolutional layers. We end up with a sequentially aggregated convolutional (SeqConv) layer that combines the benefits of both wide and deep representations by aggregating features of various depths in sequence. The proposed SeqConv serves as a drop-in replacement of regular wide convolutional layers and thus could be handily integrated into any backbone network. We apply SeqConv to widely adopted backbones including ResNet and ResNeXt, and conduct experiments for image classification on public benchmark datasets. Our ResNet based network with a model size of ResNet-50 easily surpasses the performance of the 2.35× larger ResNet-152, while our ResNeXt based model sets a new state-of-the-art accuracy on ImageNet classification for networks with similar model complexity. The code and pre-trained models of our work are publicly available at this https URL.
Automated food detection and recognition methods have been studied to enhance end-user life. However, most existing research focused on food ingredient type recognition, with little work has been done for food ingredient state recognition. Successful recognition of food ingredient state plays a significant role in handling the food ingredient by an intelligent system. In this work, we propose a new novel cascaded multi-head approach based on deep learning to simultaneously recognize the state and type of food ingredients. We trained and evaluated the proposed approach on a benchmark dataset of food ingredient images with nine different food states and 18 food types. We compared the proposed approach with a non-cascaded deep learning approach. The cascaded approach shows improvement in food ingredient state recognition with 87% accuracy compared to 81% using a non-cascaded deep learning method. Our proposed method broadly applies to various tasks where food ingredient state recognition is essential, such as feeding elderly and disabled people and automating food recognition and preparation.
